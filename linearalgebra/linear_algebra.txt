vectors
#######
- moving arrow/dot on 2d or 3d graph starting from origin
- represented as list of co-ordinates - [x,y] or [x,y,z]

vector addition
###############
- sum of 2 vectors is where we would land eventually after doing the movements one after another
- [1,2] + [3,4] = [4,6]

vector multiplication (to scalar) i.e. vector scaling
#####################################################
- multiplication of vector with a scalar is scaling the vector that many times long/small/reverse

basis vectors
#############
- i^ and j^ are basic vectors i.e. [1,0] and [0,1]

scaled basis vectors
####################
- vector [3,4] is sum of scaled basis vectors i.e. 3(i) + 4(j)

linear combination of vectors
#############################
if one vector scales:
- its tip makes a straight line i.e. linear combination

span of vectors
###############
if all vectors scale:
- 2 vectors in 2d - if both of the vectors scale, we can capture all points on the plane, be origin, be one line
- 2 vectors in 3d - span cuts through the 3d space
- 3 vectors in 3d and 3rd one not in span of first 2 - spans all 3d space i.e. linearly independent
- 3 vectors in 3d and 3rd one in span of first 2 - spans through 3d space i.e. linearly dependent

linear transformation of vectors and matrices
#############################################
- from input vector to output vector via movement
- lines remain lines and origin remains fixed i.e. grid lines remain parallel and evenly spaced
- transformation of a vector can be calculated based on where i^ and j^ land
- 3(i) + 2(j) formula remains constant
- if transformation of i^ and j^ is linearly dependent, it will squish entire 2d space in one line
- 2x2 matrix is where i^ and j^ lands
- vector is a list
- their multiplication is the transformation of vector

matrix multiplication as composition
####################################
- effect of a rotation and shear i.e. 2 transformations on a vector is second matrix*(first matrix * vector) i.e. composition of both matrices * vector
- right to left application like functions
- a b * e f = a b*e + a b*f =   a*e + b*g (one col)  a*f + b*h (second col)
  c d   g h   c d g   c d h     c     d              c     d
- M1 * M2 not equal to M2 * M1
- matrix multiplication is associative i.e. (AB)C = A(BC)

determinant of transformation
#############################
- how much area is scaled i.e. expanded/squished by transformation
- whatever happens to area of i and j will happen to larger areas also
- 1 0 has area 1
  0 1
  3 0 has area 6
  0 2
  i.e. transformation scaled the area by 6
- if all space is squished to a line or point then the determinant is 0 - then everything will be 0
- negative determinant is when orientation is inverted eg: flipping paper eg: first i^ was on right of j^ and later on left
- in 3d determinant is volume by which it is scaled
- in 3d, 0 determinant means flat plane or line or point
- for matrix a b, the determinant is a*d - b*c
             c d
- determinant of a b c is a*e f - b * d f + c * d e
                 d e f      h i       g i       g h
                 g h i
- det(m1m2) = det(m1)*det(m2)

(gaussian elimination)
(row echelon form)

inverse matrix
##############
- reversing the transformation i.e. a^-1
- M * M^-1 = doing nothing i.e starting point i.e identity matrix i.e. 1 0
                                                                       0 1
- when determinant is not 0
- M * x = v
- M^-1 * M * x = M^-1 * v
- x = v * M^-1
- when determinant is 0
- we cant mathematically inverse that transformation
- solution exists only if final vector resides on line

rank and column space and null space of matrix
##############################################
- no of dimensions in output of transformation i.e. 3,2,1
- 3 is naturally there in 3d
- 2 for 2d plane
- 1 for line
- 2 is naturally there in 2d
- 1 for line
- column space - all possible outputs of transformation
- rank is no of dimensions in column space
- full rank - when rank of matrix is equal to no of columns
- 0 vector is included in column space
- null space/kernel - space of vectors that can land on 0 vector

non square matrices
###################
- 2 * 3 or 3 * 2 or 1*2 i.e. line
- transformations between dimensions

dot products and duality
########################
- a*c=a.c + b*d
  b d
- projecting one vector on line of other and multiplying their lengths is the dot product
- when 2 vectors are perpendicular, their dot product is 0
- if vectors are in opp direction, dot product is negative
- order doesnt matter as we are just calculating length
- same as if there was a perpendicular number line in between and we were calculating projection for that i.e. where our vector will land on the line it is like the x and y axis of that number line i.e. like dot product of that perpendicular line's x,y and our vectors co-ordinates i.e. duality
- 2v * w = 2(v*w)

cross product
##############
- cross product of v and w i.e. vXw i.e. when we also take both to the tip of each other they mark their area
- if v is on right of w, cross product is positive else negative
- i.e. cross product is determiannt i.e. area along with orientation i.e. positive or negative sign of result
- 3v x w = 3(vxw)
- but, ideal cross product is 2 vectors into 3rd one - new vector's length is area of previous 2 and it will be perpendicular to parallelogram of other 2
- v1 x w1 = v2w3 - w2v3
  v2   w2   v3w1 - w3v1
  v3   w3   v1w2 - w1v2

cramer's rule
##############
- orthonomal transformations i.e. rotations preserve dot products, others dont
- 2x + -1y = 4
- 0x + 1y = 2
- 2 -1 * x = 4
  0  1   y   2
- y = det of 2 4
             0 2
             ----
      det of 2 -1
             0  1
- x = det of 4 -1
             2 1
             ----
      det of 2 -1
             0  1

change of basis
################
when someone means  1
                    2
whose basis is say 2 -1
                   1  1
to us it means matrix vector multiplication of the 2
take inverse matrix for her to understand what we mean
vector in her language * her basis i.e. vector in our language * her transformation i.e. her transformation in our language * reverse of her basis i.e. her transformation in her language
A^-1 M A

eigen vectors and eigen values
##############################
- eigen vectors are those that remain on their own span/line even after transformation i.e. they only get expanded/squished
- eigen value is factor by which it is stretched or squished
- in 3d eigen vector is the axis of rotation
- rotations have eigen value 1 - because they dont stretch or squish anything
- transformation matrix * eigen vector = eigen vector * eigen value
- TM * EVec = EVec * EValue
- TM * EVec = EVec * IM * EValue
- (TM * EVec) - (EVec * IM * EValue) = 0
- (TM - IM * EValue) * EVec = 0
- det(TM - IM * EValue) = 0
- det of a-eval    c         = 0
         b         d-eval      0
- for diagonal matrices, all basis vectors are eigen vectors with diagonal as their eigen value
- 3 0 * 3 0 * x  = 3^2*x
  0 2   0 2   y    3*2*y
- we can set eigen vectors as basis vectors i.e. eigen basis

trick for computing eigen value
###############################
m +- squarerroot(m square - p)

abstract vector spaces
######################
- f is transformation
- f+g(x) = f(x) + g(x)
- (2f)x = 2f(x)
- linear transformations preserve vector addition and scalar multiplication
- same for derivative
- 4 x^7 - 5 x^2 =
0
0
5
0
0
0
0
4
0
0
0
...